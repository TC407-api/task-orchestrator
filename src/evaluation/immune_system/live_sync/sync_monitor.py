"""
Sync Health Monitor for Graphiti Federation.

Tracks synchronization health metrics, detects stalls, and
generates alerts for sync issues across portfolio projects.
"""

import logging
import threading
import time
from collections import deque
from dataclasses import dataclass, field
from enum import Enum
from typing import Any, Deque, Dict, List, Optional

logger = logging.getLogger(__name__)

# Configuration defaults
DEFAULT_STALL_THRESHOLD_SEC = 300.0  # 5 minutes without update = Stall
DEFAULT_LATENCY_DEGRADED_MS = 1000.0  # >1s latency = Degraded
DEFAULT_LATENCY_CRITICAL_MS = 5000.0  # >5s latency = Critical
MAX_CONSECUTIVE_FAILURES = 3  # >3 fails = Critical
HISTORY_WINDOW_SIZE = 50  # Keep last N latency points


class SyncStatus(Enum):
    """Enumeration of possible sync health states."""
    HEALTHY = "healthy"
    DEGRADED = "degraded"
    CRITICAL = "critical"
    UNKNOWN = "unknown"


@dataclass
class SyncAlert:
    """Represents an actionable alert generated by the monitor."""
    project_id: str
    severity: SyncStatus
    message: str
    timestamp: float = field(default_factory=time.time)


@dataclass
class ProjectSyncState:
    """Internal state tracking for a project's synchronization."""
    project_id: str
    last_attempt_timestamp: float = 0.0
    last_success_timestamp: float = 0.0
    current_latency_ms: float = 0.0
    consecutive_failures: int = 0
    total_failures: int = 0
    total_syncs: int = 0
    health_status: SyncStatus = SyncStatus.UNKNOWN
    latency_history: Deque[float] = field(
        default_factory=lambda: deque(maxlen=HISTORY_WINDOW_SIZE)
    )
    last_error_message: Optional[str] = None


class SyncHealthMonitor:
    """
    Monitors the real-time synchronization health of Graphiti federation projects.

    Features:
    - Thread-safe metric recording
    - Latency tracking and history
    - Stall detection (heartbeat monitoring)
    - Automatic health classification
    - Alert generation
    """

    def __init__(
        self,
        stall_threshold_sec: float = DEFAULT_STALL_THRESHOLD_SEC,
        latency_critical_ms: float = DEFAULT_LATENCY_CRITICAL_MS
    ):
        """
        Initialize the SyncHealthMonitor.

        Args:
            stall_threshold_sec: Seconds before a project is considered stalled.
            latency_critical_ms: Latency threshold in ms to consider critical.
        """
        self._projects: Dict[str, ProjectSyncState] = {}
        self._lock = threading.RLock()

        self.stall_threshold = stall_threshold_sec
        self.latency_critical = latency_critical_ms
        self.latency_degraded = DEFAULT_LATENCY_DEGRADED_MS

        logger.info(
            f"SyncHealthMonitor initialized "
            f"(Stall: {stall_threshold_sec}s, Crit Latency: {latency_critical_ms}ms)"
        )

    def _get_or_create_project(self, project_id: str) -> ProjectSyncState:
        """Retrieve project state or initialize if new."""
        if project_id not in self._projects:
            self._projects[project_id] = ProjectSyncState(project_id=project_id)
        return self._projects[project_id]

    def record_sync_success(self, project_id: str, latency_ms: float) -> None:
        """
        Record a successful synchronization event.

        Args:
            project_id: The unique identifier of the project.
            latency_ms: The time taken for the sync operation in milliseconds.
        """
        with self._lock:
            state = self._get_or_create_project(project_id)
            now = time.time()

            state.last_attempt_timestamp = now
            state.last_success_timestamp = now
            state.current_latency_ms = latency_ms
            state.consecutive_failures = 0
            state.total_syncs += 1
            state.latency_history.append(latency_ms)

            self._evaluate_project_health(state)

            logger.debug(f"Recorded sync success for {project_id}: {latency_ms:.2f}ms")

    def record_sync_failure(self, project_id: str, error_msg: str) -> None:
        """
        Record a failed synchronization attempt.

        Args:
            project_id: The unique identifier of the project.
            error_msg: Description of the error.
        """
        with self._lock:
            state = self._get_or_create_project(project_id)
            state.last_attempt_timestamp = time.time()
            state.consecutive_failures += 1
            state.total_failures += 1
            state.last_error_message = error_msg

            self._evaluate_project_health(state)

            logger.warning(f"Recorded sync failure for {project_id}: {error_msg}")

    def _evaluate_project_health(self, state: ProjectSyncState) -> None:
        """Determine the health status of a project."""
        now = time.time()
        time_since_last_success = now - state.last_success_timestamp

        # Critical Checks
        if state.consecutive_failures >= MAX_CONSECUTIVE_FAILURES:
            state.health_status = SyncStatus.CRITICAL
            return

        if (
            state.last_success_timestamp > 0
            and time_since_last_success > self.stall_threshold
        ):
            state.health_status = SyncStatus.CRITICAL
            return

        if state.current_latency_ms > self.latency_critical:
            state.health_status = SyncStatus.CRITICAL
            return

        # Degraded Checks
        if state.current_latency_ms > self.latency_degraded:
            state.health_status = SyncStatus.DEGRADED
            return

        if state.consecutive_failures > 0:
            state.health_status = SyncStatus.DEGRADED
            return

        # Healthy
        state.health_status = SyncStatus.HEALTHY

    def check_health_and_alert(self) -> List[SyncAlert]:
        """
        Periodic check method. Detects stalls and generates alerts.

        Returns:
            List of active alerts.
        """
        alerts: List[SyncAlert] = []

        with self._lock:
            for pid, state in self._projects.items():
                self._evaluate_project_health(state)

                if state.health_status == SyncStatus.CRITICAL:
                    msg = self._generate_failure_message(state)
                    alerts.append(SyncAlert(pid, SyncStatus.CRITICAL, msg))

                elif state.health_status == SyncStatus.DEGRADED:
                    msg = f"Degraded performance. Latency: {state.current_latency_ms:.2f}ms"
                    alerts.append(SyncAlert(pid, SyncStatus.DEGRADED, msg))

        return alerts

    def _generate_failure_message(self, state: ProjectSyncState) -> str:
        """Generate human-readable failure reasons."""
        now = time.time()
        if state.consecutive_failures >= MAX_CONSECUTIVE_FAILURES:
            return (
                f"Too many consecutive failures ({state.consecutive_failures}). "
                f"Last error: {state.last_error_message}"
            )

        if (
            state.last_success_timestamp > 0
            and (now - state.last_success_timestamp > self.stall_threshold)
        ):
            return (
                f"Sync STALLED. No update for "
                f"{int(now - state.last_success_timestamp)} seconds."
            )

        if state.current_latency_ms > self.latency_critical:
            return f"Critical Latency: {state.current_latency_ms:.2f}ms"

        return "Unknown critical error"

    def get_dashboard_metrics(self) -> Dict[str, Any]:
        """
        Returns a snapshot of metrics for the immune_dashboard.

        Returns:
            Dictionary with global stats and per-project details.
        """
        with self._lock:
            total_projects = len(self._projects)
            healthy_count = sum(
                1 for p in self._projects.values()
                if p.health_status == SyncStatus.HEALTHY
            )
            critical_count = sum(
                1 for p in self._projects.values()
                if p.health_status == SyncStatus.CRITICAL
            )

            project_details = []
            for pid, state in self._projects.items():
                avg_latency = (
                    sum(state.latency_history) / len(state.latency_history)
                    if state.latency_history else 0.0
                )

                project_details.append({
                    "project_id": pid,
                    "status": state.health_status.value,
                    "latency_ms": round(state.current_latency_ms, 2),
                    "avg_latency_window_ms": round(avg_latency, 2),
                    "last_sync_ago_sec": round(
                        time.time() - state.last_success_timestamp, 1
                    ) if state.last_success_timestamp else -1,
                    "failures_consecutive": state.consecutive_failures,
                    "failures_total": state.total_failures
                })

            return {
                "timestamp": time.time(),
                "summary": {
                    "total_projects": total_projects,
                    "healthy": healthy_count,
                    "critical": critical_count,
                    "degraded": total_projects - healthy_count - critical_count
                },
                "projects": project_details
            }

    def get_project_status(self, project_id: str) -> Optional[Dict[str, Any]]:
        """Get status for a specific project."""
        with self._lock:
            state = self._projects.get(project_id)
            if not state:
                return None

            return {
                "project_id": state.project_id,
                "health_status": state.health_status.value,
                "latency_ms": state.current_latency_ms,
                "consecutive_failures": state.consecutive_failures,
                "total_syncs": state.total_syncs,
                "last_error": state.last_error_message,
            }


__all__ = [
    "SyncHealthMonitor",
    "SyncStatus",
    "SyncAlert",
    "ProjectSyncState",
]
